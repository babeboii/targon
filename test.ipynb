{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtensor = bt.subtensor( network=\"finney\" )\n",
    "metagraph = bt.metagraph( netuid=4, lite=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "slop = metagraph.weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_averaged_scores = torch.zeros((metagraph.n))\n",
    "moving_averaged_scores[0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the tensors\n",
    "def merge_tensors(tensor_a, tensor_b):\n",
    "    assert tensor_a.shape == tensor_b.shape, \"Tensors must be of the same shape\"\n",
    "\n",
    "    merged_tensor = torch.max(tensor_a, tensor_b)  # This will automatically choose the non-zero value if one is zero\n",
    "    # Handling NaN values\n",
    "    nan_mask = torch.isnan(tensor_a) | torch.isnan(tensor_b)\n",
    "    merged_tensor[nan_mask] = torch.max(tensor_a[nan_mask], tensor_b[nan_mask])\n",
    "\n",
    "    return merged_tensor\n",
    "\n",
    "def merge_tensors_with_penalty(tensor_a, tensor_b, penalty=0.15):\n",
    "    assert tensor_a.shape == tensor_b.shape, \"Tensors must be of the same shape\"\n",
    "\n",
    "    merged_tensor = torch.zeros_like(tensor_a)\n",
    "    for i in range(tensor_a.numel()):\n",
    "        if tensor_a[i] == 0 and tensor_b[i] != 0:\n",
    "            merged_tensor[i] = tensor_b[i] * (1 - penalty)  # Apply penalty if tensor_a is zero\n",
    "        elif tensor_b[i] == 0 and tensor_a[i] != 0:\n",
    "            merged_tensor[i] = tensor_a[i] * (1 - penalty)  # Apply penalty if tensor_b is zero\n",
    "        else:\n",
    "            merged_tensor[i] = max(tensor_a[i], tensor_b[i])  # Use the larger value if neither is zero\n",
    "\n",
    "    return merged_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.5000e-01, 3.5567e-03, 3.5097e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 3.7096e-03, 0.0000e+00, 4.4376e-03, 3.0557e-03, 3.8099e-03,\n",
       "        3.1530e-03, 0.0000e+00, 3.9746e-03, 3.9204e-03, 4.0782e-03, 3.9876e-03,\n",
       "        4.0549e-03, 4.6677e-03, 3.5251e-03, 3.4439e-03, 3.7391e-03, 3.8317e-03,\n",
       "        3.8370e-03, 4.4617e-03, 3.1802e-03, 4.8448e-03, 3.6456e-03, 3.9778e-03,\n",
       "        3.5057e-03, 3.8754e-03, 3.3530e-03, 3.4951e-03, 3.0738e-03, 3.8201e-03,\n",
       "        3.9983e-03, 3.3591e-03, 4.0271e-03, 3.3911e-03, 4.3194e-03, 4.0390e-03,\n",
       "        4.1021e-03, 2.9113e-03, 3.7015e-03, 3.5096e-03, 3.7402e-03, 3.9607e-03,\n",
       "        3.0503e-03, 4.3011e-03, 3.4560e-03, 3.4347e-03, 2.0604e-03, 0.0000e+00,\n",
       "        3.9381e-03, 4.5924e-03, 3.2041e-03, 3.8474e-03, 3.6344e-03, 4.0811e-03,\n",
       "        4.4490e-03, 2.5993e-03, 4.0878e-03, 4.1143e-03, 4.8810e-03, 3.2216e-03,\n",
       "        4.4295e-03, 5.1169e-03, 3.8160e-03, 4.1332e-03, 4.0693e-03, 0.0000e+00,\n",
       "        3.6838e-03, 4.0361e-03, 0.0000e+00, 4.3268e-03, 3.1472e-03, 3.3575e-03,\n",
       "        3.7721e-03, 3.7415e-03, 3.2655e-03, 4.1522e-03, 4.0722e-03, 0.0000e+00,\n",
       "        2.3979e-03, 3.4361e-03, 3.6180e-03, 3.4777e-03, 3.8359e-03, 0.0000e+00,\n",
       "        4.3134e-03, 3.7710e-03, 3.9689e-03, 4.0757e-03, 2.9346e-03, 2.9868e-03,\n",
       "        1.2362e-03, 3.6075e-03, 4.0255e-03, 3.9693e-03, 2.7635e-03, 3.6116e-03,\n",
       "        3.5641e-03, 3.3059e-03, 3.0638e-03, 3.7492e-03, 3.3831e-03, 4.3960e-03,\n",
       "        3.0121e-03, 4.0667e-03, 3.5124e-03, 3.2623e-03, 3.3522e-03, 4.2740e-03,\n",
       "        2.9089e-03, 3.5117e-03, 3.7159e-03, 4.7571e-03, 2.8197e-03, 3.0919e-03,\n",
       "        3.6199e-03, 4.4843e-03, 2.8498e-03, 3.3565e-03, 4.2871e-03, 4.3836e-03,\n",
       "        3.0781e-03, 3.5756e-03, 4.9583e-03, 3.2283e-03, 3.8225e-03, 3.4933e-03,\n",
       "        3.6009e-03, 4.1247e-03, 3.7555e-03, 4.0921e-03, 3.8878e-03, 3.3880e-03,\n",
       "        3.4950e-03, 0.0000e+00, 4.1870e-03, 3.1820e-03, 4.7409e-03, 3.2664e-03,\n",
       "        4.6406e-03, 4.3651e-03, 3.1102e-03, 3.7532e-03, 4.2707e-03, 3.9972e-03,\n",
       "        0.0000e+00, 3.9507e-03, 3.8803e-03, 3.4576e-03, 3.9001e-03, 4.2488e-03,\n",
       "        0.0000e+00, 3.6209e-03, 3.2025e-03, 3.1736e-03, 4.5680e-03, 3.8088e-03,\n",
       "        3.5593e-03, 4.1347e-03, 3.6755e-03, 3.9825e-03, 3.3640e-03, 3.7224e-03,\n",
       "        3.2736e-03, 3.3753e-03, 3.2948e-03, 0.0000e+00, 3.0545e-03, 4.3686e-03,\n",
       "        1.8380e-04, 0.0000e+00, 4.0515e-03, 3.3493e-03, 3.5981e-03, 3.1310e-03,\n",
       "        4.3043e-03, 4.0705e-03, 3.5444e-03, 3.3528e-03, 2.8855e-03, 0.0000e+00,\n",
       "        4.6032e-03, 0.0000e+00, 4.6064e-03, 3.7807e-03, 3.4410e-03, 3.1717e-03,\n",
       "        2.6413e-03, 3.6050e-03, 3.5747e-03, 4.5307e-03, 3.8262e-03, 4.1480e-03,\n",
       "        4.0982e-03, 3.7902e-03, 3.2045e-03, 0.0000e+00, 3.4728e-03, 3.6946e-03,\n",
       "        4.2584e-03, 3.2907e-03, 4.3967e-03, 4.1798e-03, 0.0000e+00, 3.4024e-03,\n",
       "        3.9842e-03, 3.2005e-03, 2.7945e-03, 3.0111e-03, 2.7714e-03, 3.5694e-03,\n",
       "        4.4525e-03, 3.8300e-03, 4.9615e-03, 0.0000e+00, 3.5795e-03, 3.9649e-03,\n",
       "        0.0000e+00, 0.0000e+00, 3.4635e-03, 3.7844e-03, 2.6435e-03, 3.6724e-03,\n",
       "        4.4997e-03, 3.9445e-03, 3.5382e-03, 3.7065e-03, 3.6558e-03, 4.4120e-03,\n",
       "        3.8761e-03, 3.4627e-03, 3.5660e-03, 0.0000e+00, 3.5717e-03, 3.0332e-03,\n",
       "        3.3960e-03, 3.5357e-03, 3.7742e-03, 3.8609e-03, 3.1513e-03, 4.2727e-03,\n",
       "        3.7792e-03, 3.7073e-03, 4.2265e-03, 3.9646e-03, 3.4150e-03, 0.0000e+00,\n",
       "        3.5832e-03, 4.4033e-03, 3.4219e-03, 0.0000e+00])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(moving_averaged_scores)):\n",
    "    if moving_averaged_scores[i] == 0 and slop[i] != 0:\n",
    "        moving_averaged_scores[i] = slop[i] * 0.85  # Applying 15% penalty\n",
    "\n",
    "\n",
    "# Merging with penalty\n",
    "moving_averaged_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
